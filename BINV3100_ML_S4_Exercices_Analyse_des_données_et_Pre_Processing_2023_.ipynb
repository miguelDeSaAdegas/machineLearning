{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/miguelDeSaAdegas/machineLearning/blob/main/BINV3100_ML_S4_Exercices_Analyse_des_donn%C3%A9es_et_Pre_Processing_2023_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AL83i7MKcPLX"
      },
      "source": [
        "#BINV3100 ML S4 : Théorie\n",
        "\n",
        "##Vidéos obligatoires\n",
        "Nous vous demandons de regarder et de compredre les vidéos suivante:\n",
        "\n",
        "1.   [PRE-PROCESSING](https://www.youtube.com/watch?v=OGWwzm304Xs&list=PLO_fdPEVlfKqMDNmCFzQISI2H_nJcEDJq&index=24)\n",
        "2.   [IMPUTER : NETTOYAGE DE DONNÉES](https://www.youtube.com/watch?v=QVEJJNsz-eM&list=PLO_fdPEVlfKqMDNmCFzQISI2H_nJcEDJq&index=26)\n",
        "3.   [EXPLORATION DES DONNÉES](https://www.youtube.com/watch?v=u64sWJEP4S0&list=PLO_fdPEVlfKqMDNmCFzQISI2H_nJcEDJq&index=31)\n",
        "\n",
        "\n",
        "A la fin de chaque vidéo, un exercice est proposé. Vous retrouverez ces exercices dans la Partie 1 des exercices de cette semaine\n",
        "\n",
        "## La vidéo suivante est en bonus:\n",
        "\n",
        "6.  [PIPELINE AVANCÉE](https://www.youtube.com/watch?v=41mnga4ptso&list=PLO_fdPEVlfKqMDNmCFzQISI2H_nJcEDJq&index=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Js5X-FAlxHxg"
      },
      "source": [
        "# Partie 1: Exercices relatifs aux Vidéos Machine Learnia\n",
        "\n",
        "[Lien vers la playlist Machine Learnia](https://www.youtube.com/playlist?list=PLO_fdPEVlfKqMDNmCFzQISI2H_nJcEDJq)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gN0ixxrp6NWi"
      },
      "source": [
        "## Vidéos (22 et 23 /30) :\n",
        "\n",
        "[MÉTRIQUE DE RÉGRESSION](https://www.youtube.com/watch?v=_TE9fDgtOaE&list=PLO_fdPEVlfKqMDNmCFzQISI2H_nJcEDJq&index=23)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17UzKOPECNCO"
      },
      "source": [
        "## Vidéos (24 à 26/30):\n",
        "\n",
        "[PRE-PROCESSING](https://www.youtube.com/watch?v=OGWwzm304Xs&list=PLO_fdPEVlfKqMDNmCFzQISI2H_nJcEDJq&index=24)\n",
        "\n",
        "(Bonus) [PIPELINE AVANCÉE](https://www.youtube.com/watch?v=41mnga4ptso&list=PLO_fdPEVlfKqMDNmCFzQISI2H_nJcEDJq&index=25)\n",
        "\n",
        "[IMPUTER : NETTOYAGE DE DONNÉES](https://www.youtube.com/watch?v=QVEJJNsz-eM&list=PLO_fdPEVlfKqMDNmCFzQISI2H_nJcEDJq&index=26)\n",
        "\n",
        "Vous allez tester les différents Scaler sur le dataset des iris.\n",
        "\n",
        "Pour ce faire\n",
        "<ol>\n",
        "<li> Importer le dataset des iris </li>\n",
        "<li> Créez un Train Set et un Test Set avec le Test Set représentant 25% des données et random_State égal à 4 </li>\n",
        "<li> Pour chacun des 3 scalers (StandardScaler(), MinMaxScaler et RobustScaler)</li>\n",
        "<ol type=\"a\">\n",
        "<li>Créez un pipeline avec un PolynomialFeatures et un SGDClassifier (avec un random_state égal à 0 pour le SGDClassifier) </li>\n",
        "<font color='red'><strong>Attention ! Renseignez-vous bien sur l'ordre dans lequel il faut placer le Scaler et le PolynomialFeatures !</strong> </font>\n",
        "<li>Cherchez les meilleurs paramètres à l'aide de GridSearchCV testant 2,3,4 pour le dégré du PolynomialFeatures, et \"l1\", \"l2\" pour la pénalité du SGDClassifier</li>\n",
        "<li>Afficher les meilleurs paramètres trouvé, le meilleur score ainsi que le score sur les données de test </li>\n",
        "</ol>\n",
        "<li>Sur base des résultats affichés au point 3., Quel scaler choisiriez-vous ? Justifiez !!</li>\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ybrn0Ct6d9Ur",
        "outputId": "a4d24a0e-f4c8-4619-d347-3f62202b2078",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "from sklearn.impute import SimpleImputer,KNNImputer\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=4)\n",
        "\n",
        "params = {\n",
        "    'polynomialfeatures__degree' : [2, 3, 4],\n",
        "    'sgdclassifier__penalty' : ['l1', 'l2']\n",
        "}\n",
        "\n",
        "\n",
        "###STANDARD\n",
        "pipelineStandard = make_pipeline(PolynomialFeatures(),StandardScaler(),SGDClassifier(random_state=0))\n",
        "\n",
        "gridStandard = GridSearchCV(pipelineStandard, params, cv=5)\n",
        "gridStandard.fit(X_train, y_train)\n",
        "\n",
        "print(\"Standard\")\n",
        "print(gridStandard.best_params_)\n",
        "print(gridStandard.best_score_)\n",
        "print(gridStandard.score(X_test, y_test))\n",
        "\n",
        "###MINMAX\n",
        "pipelineMinmax = make_pipeline(PolynomialFeatures(),MinMaxScaler(),SGDClassifier(random_state=0))\n",
        "\n",
        "gridMinmax = GridSearchCV(pipelineMinmax, params, cv=5)\n",
        "gridMinmax.fit(X_train, y_train)\n",
        "\n",
        "print(\"MinMax\")\n",
        "print(gridMinmax.best_params_)\n",
        "print(gridMinmax.best_score_)\n",
        "print(gridMinmax.score(X_test, y_test))\n",
        "\n",
        "###ROBUST\n",
        "pipelineRobust = make_pipeline(PolynomialFeatures(),RobustScaler(),SGDClassifier(random_state=0))\n",
        "\n",
        "gridRobust = GridSearchCV(pipelineRobust, params, cv=5)\n",
        "gridRobust.fit(X_train, y_train)\n",
        "\n",
        "print('Robuste')\n",
        "print(gridRobust.best_params_)\n",
        "print(gridRobust.best_score_)\n",
        "print(gridRobust.score(X_test, y_test))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standard\n",
            "{'polynomialfeatures__degree': 4, 'sgdclassifier__penalty': 'l1'}\n",
            "0.982213438735178\n",
            "0.9210526315789473\n",
            "MinMax\n",
            "{'polynomialfeatures__degree': 4, 'sgdclassifier__penalty': 'l2'}\n",
            "0.982213438735178\n",
            "0.9473684210526315\n",
            "Robuste\n",
            "{'polynomialfeatures__degree': 3, 'sgdclassifier__penalty': 'l1'}\n",
            "0.982213438735178\n",
            "0.9473684210526315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xEq3wcYIvfg"
      },
      "source": [
        "Réponse au 4. : le RobustScaler car il a besoin de moins de donner pour atteindre le même niveau de performance que les autres (degré 3 au lieu de degré 4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzqxLKjXOknb"
      },
      "source": [
        "# Partie 2 : Exercices Machine Learning semaine 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aApsbbFQ_eH"
      },
      "source": [
        "## Exercice 1 : Survivant du titanic :\n",
        "\n",
        "<ol>\n",
        "<li> Importez le dataset du titanic à l'aide de seaborn </li>\n",
        "<li> Gardez uniquement les colonnes 'pclass', 'age' et 'sex' comme features et la colonne 'survived' comme target </li>\n",
        "<li> Créez un Train Set et un Test set avec 20% des données pour le test set et un random_state égal à 4</li>\n",
        "<li> Créez un preprocesser qui </li>\n",
        "<ul>\n",
        "<li> pour la colonne 'pclass' remplace les valeurs manquantes en utilisant un KNNImputer() considérant les 2 plus proches voisins pour ensuite appliquer un OneHotEncoder </li>\n",
        "<li> pour la colonne 'age' remplace les valeurs manquantes par la moyenne et ensuite applique ensuite un RobustScaler </li>\n",
        "<li> pour la colonne 'sex' remplace les valeurs manquantes par la valeur la plus fréquente pour ensuite appliquer un OneHotEncoder</li>\n",
        "</ul>\n",
        "(Astuce : utilisez make_column_transformer)\n",
        "<li> Obtenez votre modèle en faisant un pipeline constitué du preprocesser suivi d'un SGDClassifier avec un random_state égal à 4 </li>\n",
        "<li> Cherchez les meilleurs paramètres du SGDClassifier à l'aide d'un GridSearchCV testant 'l1' et 'l2' comme pénalité. </li>\n",
        "<li> Afficher le meilleur paramètre, le meilleur score et le score obtenu sur les données de test.  </li>\n",
        "<li> Essayer d'améliorer le score obtenu en 7., en essayant d'autre encoder, scaler, classifier, ...\n",
        "</ol>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3GdotomCEON"
      },
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.preprocessing import StandardScaler,OneHotEncoder,RobustScaler\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.compose import make_column_transformer\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercice 2 : Emprunteurs à risques\n",
        "Une banque désire un modèle permettant de déterminer si un client emprunteur est à risque ou pas.\n",
        "<ol>\n",
        "<li> Importez le dataset customer_data.csv dans lequel la target est la colonne \"label\".</li>\n",
        "<li> Retirer la colonne \"id\".  </li>\n",
        "<li> Déterminez le pourcentage de valeurs manquantes dans chaque colonne. </li>\n",
        "<li> Sur base du résultat du point précédent, traitez les valeurs manquantes. Justifier le choix que vous avez fait pour ces valeurs :\n",
        "\n",
        "<strong> Justification : </strong>  </li>\n",
        "<li> Dessinez la heatmap de la matrice de corrélation des features et de la target en affichant la valeur des coefficents de corrélation. </li>\n",
        "<li> Sur base de la heatmap, déterminer d'une part si la target est corrélée linéairement avec une ou plusieurs features et d'autre part les features les plus corrélées linéairement entre elles.\n",
        "\n",
        "<strong> Réponse : <strong>\n",
        "</ol>"
      ],
      "metadata": {
        "id": "u4iAMuWV0md3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "UVPeQCl70kmu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}